# Attention is all you need

This is the encoder decoder code for the attention model present in the "Attention is all you need paper"


Loss values for the transalation transformer model is

0
Processing Epoch 00: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=5.687]
1
Processing Epoch 01: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.515]
2
Processing Epoch 02: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.814]
3
Processing Epoch 03: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.122]
4
Processing Epoch 04: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=4.846]
5
Processing Epoch 05: 100%|██████████| 1819/1819 [09:08<00:00,  3.31it/s, loss=4.133]
6
Processing Epoch 06: 100%|██████████| 1819/1819 [09:08<00:00,  3.31it/s, loss=4.232]
7
Processing Epoch 07: 100%|██████████| 1819/1819 [09:10<00:00,  3.31it/s, loss=4.220]
8
Processing Epoch 08: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=3.758]
9
Processing Epoch 09: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=3.548]
