# Attention is all you need

This is the encoder-decoder code for the attention model present in the "Attention is all you need paper"


Loss values for the translation transformer model is:


Processing Epoch 00: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=5.687]

Processing Epoch 01: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.515]

Processing Epoch 02: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.814]

Processing Epoch 03: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=5.122]

Processing Epoch 04: 100%|██████████| 1819/1819 [09:07<00:00,  3.32it/s, loss=4.846]

Processing Epoch 05: 100%|██████████| 1819/1819 [09:08<00:00,  3.31it/s, loss=4.133]

Processing Epoch 06: 100%|██████████| 1819/1819 [09:08<00:00,  3.31it/s, loss=4.232]

Processing Epoch 07: 100%|██████████| 1819/1819 [09:10<00:00,  3.31it/s, loss=4.220]

Processing Epoch 08: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=3.758]

Processing Epoch 09: 100%|██████████| 1819/1819 [09:09<00:00,  3.31it/s, loss=3.548]
